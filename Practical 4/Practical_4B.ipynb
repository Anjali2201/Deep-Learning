{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 4B - MNIST Classification with a ANN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras.datasets.mnist as mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Base Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the base layer\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the Dense layer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Layer_Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size,input_size)\n",
    "        self.biases = np.random.randn(output_size,1)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(self.weights, self.input ) + self.biases\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        weights_gradient = np.dot(output_gradient,self.input.T )\n",
    "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        self.biases -=learning_rate * output_gradient\n",
    "        return input_gradient"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Activation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the activation layer\n",
    "\n",
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the activation functions\n",
    "\n",
    "# Linear function\n",
    "class Linear(Activation):\n",
    "    def __init__(self):\n",
    "        def linear(x):\n",
    "            return x\n",
    "        \n",
    "        def linear_prime(x):\n",
    "            return 1\n",
    "        \n",
    "        super().__init__(linear, linear_prime)\n",
    "\n",
    "# Tanh function\n",
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        def tanh(x):\n",
    "            return np.tanh(x)\n",
    "        \n",
    "        def tanh_prime(x):\n",
    "            return 1 - np.tanh(x)**2\n",
    "        \n",
    "        super().__init__(tanh, tanh_prime)\n",
    "\n",
    "# Sigmoid function\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        def sigmoid_prime(x):\n",
    "            return sigmoid(x) * (1 - sigmoid(x))\n",
    "        \n",
    "        super().__init__(sigmoid, sigmoid_prime)\n",
    "\n",
    "# ReLU function\n",
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        def relu(x):\n",
    "            return np.maximum(0,x)\n",
    "        \n",
    "        def relu_prime(x):\n",
    "            return np.where(x > 0, 1, 0)\n",
    "        \n",
    "        super().__init__(relu, relu_prime)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the loss function\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "# creating the loss function derivative\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / np.size(y_true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the function to train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, input):\n",
    "    output = input\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    return output\n",
    "\n",
    "def train(network, loss, loss_prime,x_train, y_train, epochs=10, learning_rate=0.1):\n",
    "    for e in range (epochs):\n",
    "        error = 0\n",
    "        for x,y in zip(x_train, y_train):\n",
    "            output = predict(network, x)\n",
    "            error += loss(y, output)\n",
    "            output_gradient = loss_prime(y, output)\n",
    "            for layer in reversed(network):\n",
    "                output_gradient = layer.backward(output_gradient, learning_rate)\n",
    "\n",
    "        error  /=len(x_train)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Epoch: %d, Error: %.3f\" % (e, error))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data (x,y,limit):\n",
    "    x = x.reshape(x.shape[0], 28*28, 1)\n",
    "    x = x.astype('float32')/255\n",
    "\n",
    "    y = np_utils.to_categorical(y, 10)\n",
    "    y  = y.reshape(y.shape[0], 10, 1)\n",
    "\n",
    "    x_train = x[:limit]\n",
    "    y_train = y[:limit]\n",
    "\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 5000)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Error: 0.129\n",
      "Epoch: 1, Error: 0.088\n",
      "Epoch: 2, Error: 0.079\n",
      "Epoch: 3, Error: 0.073\n",
      "Epoch: 4, Error: 0.069\n",
      "Epoch: 5, Error: 0.066\n",
      "Epoch: 6, Error: 0.063\n",
      "Epoch: 7, Error: 0.061\n",
      "Epoch: 8, Error: 0.058\n",
      "Epoch: 9, Error: 0.055\n",
      "Epoch: 10, Error: 0.053\n",
      "Epoch: 11, Error: 0.051\n",
      "Epoch: 12, Error: 0.050\n",
      "Epoch: 13, Error: 0.048\n",
      "Epoch: 14, Error: 0.047\n",
      "Epoch: 15, Error: 0.046\n",
      "Epoch: 16, Error: 0.045\n",
      "Epoch: 17, Error: 0.045\n",
      "Epoch: 18, Error: 0.044\n",
      "Epoch: 19, Error: 0.043\n",
      "Epoch: 20, Error: 0.042\n",
      "Epoch: 21, Error: 0.042\n",
      "Epoch: 22, Error: 0.041\n",
      "Epoch: 23, Error: 0.040\n",
      "Epoch: 24, Error: 0.040\n",
      "Epoch: 25, Error: 0.039\n",
      "Epoch: 26, Error: 0.039\n",
      "Epoch: 27, Error: 0.038\n",
      "Epoch: 28, Error: 0.038\n",
      "Epoch: 29, Error: 0.037\n",
      "Epoch: 30, Error: 0.037\n",
      "Epoch: 31, Error: 0.037\n",
      "Epoch: 32, Error: 0.036\n",
      "Epoch: 33, Error: 0.036\n",
      "Epoch: 34, Error: 0.036\n",
      "Epoch: 35, Error: 0.036\n",
      "Epoch: 36, Error: 0.035\n",
      "Epoch: 37, Error: 0.035\n",
      "Epoch: 38, Error: 0.035\n",
      "Epoch: 39, Error: 0.034\n",
      "Epoch: 40, Error: 0.034\n",
      "Epoch: 41, Error: 0.034\n",
      "Epoch: 42, Error: 0.034\n",
      "Epoch: 43, Error: 0.034\n",
      "Epoch: 44, Error: 0.033\n",
      "Epoch: 45, Error: 0.033\n",
      "Epoch: 46, Error: 0.033\n",
      "Epoch: 47, Error: 0.033\n",
      "Epoch: 48, Error: 0.033\n",
      "Epoch: 49, Error: 0.032\n",
      "Epoch: 50, Error: 0.032\n",
      "Epoch: 51, Error: 0.032\n",
      "Epoch: 52, Error: 0.032\n",
      "Epoch: 53, Error: 0.032\n",
      "Epoch: 54, Error: 0.031\n",
      "Epoch: 55, Error: 0.031\n",
      "Epoch: 56, Error: 0.031\n",
      "Epoch: 57, Error: 0.031\n",
      "Epoch: 58, Error: 0.031\n",
      "Epoch: 59, Error: 0.031\n",
      "Epoch: 60, Error: 0.031\n",
      "Epoch: 61, Error: 0.030\n",
      "Epoch: 62, Error: 0.030\n",
      "Epoch: 63, Error: 0.030\n",
      "Epoch: 64, Error: 0.030\n",
      "Epoch: 65, Error: 0.030\n",
      "Epoch: 66, Error: 0.030\n",
      "Epoch: 67, Error: 0.030\n",
      "Epoch: 68, Error: 0.030\n",
      "Epoch: 69, Error: 0.030\n",
      "Epoch: 70, Error: 0.030\n",
      "Epoch: 71, Error: 0.029\n",
      "Epoch: 72, Error: 0.029\n",
      "Epoch: 73, Error: 0.029\n",
      "Epoch: 74, Error: 0.029\n",
      "Epoch: 75, Error: 0.029\n",
      "Epoch: 76, Error: 0.029\n",
      "Epoch: 77, Error: 0.029\n",
      "Epoch: 78, Error: 0.029\n",
      "Epoch: 79, Error: 0.029\n",
      "Epoch: 80, Error: 0.029\n",
      "Epoch: 81, Error: 0.029\n",
      "Epoch: 82, Error: 0.029\n",
      "Epoch: 83, Error: 0.029\n",
      "Epoch: 84, Error: 0.028\n",
      "Epoch: 85, Error: 0.028\n",
      "Epoch: 86, Error: 0.028\n",
      "Epoch: 87, Error: 0.028\n",
      "Epoch: 88, Error: 0.028\n",
      "Epoch: 89, Error: 0.028\n",
      "Epoch: 90, Error: 0.028\n",
      "Epoch: 91, Error: 0.028\n",
      "Epoch: 92, Error: 0.028\n",
      "Epoch: 93, Error: 0.028\n",
      "Epoch: 94, Error: 0.028\n",
      "Epoch: 95, Error: 0.028\n",
      "Epoch: 96, Error: 0.028\n",
      "Epoch: 97, Error: 0.028\n",
      "Epoch: 98, Error: 0.028\n",
      "Epoch: 99, Error: 0.028\n"
     ]
    }
   ],
   "source": [
    "# creating the network\n",
    "# dense (28*28,100)\n",
    "# activation (tanh)\n",
    "# dense (100,10)\n",
    "# activation (sigmoid)\n",
    "network = [\n",
    "    Layer_Dense(28*28, 40),\n",
    "    Tanh(),\n",
    "    Layer_Dense(40, 10),\n",
    "    Sigmoid()\n",
    "]\n",
    "\n",
    "\n",
    "# training the network\n",
    "train(network, mse, mse_prime, x_train, y_train, epochs=100, learning_rate=0.1)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 7, Predicted: 7\n",
      "Original: 2, Predicted: 2\n",
      "Original: 1, Predicted: 1\n",
      "Original: 0, Predicted: 0\n",
      "Original: 4, Predicted: 4\n",
      "Original: 1, Predicted: 1\n",
      "Original: 4, Predicted: 4\n",
      "Original: 9, Predicted: 5\n",
      "Original: 5, Predicted: 0\n",
      "Original: 9, Predicted: 7\n",
      "Original: 0, Predicted: 0\n",
      "Original: 6, Predicted: 6\n",
      "Original: 9, Predicted: 4\n",
      "Original: 0, Predicted: 0\n",
      "Original: 1, Predicted: 1\n",
      "Original: 5, Predicted: 3\n",
      "Original: 9, Predicted: 0\n",
      "Original: 7, Predicted: 7\n",
      "Original: 3, Predicted: 6\n",
      "Original: 4, Predicted: 4\n"
     ]
    }
   ],
   "source": [
    "# testing the network\n",
    "for x,y in zip(x_test, y_test):\n",
    "    output = predict(network, x)\n",
    "    print(\"Original: %d, Predicted: %d\" % (np.argmax(y), np.argmax(output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.65\n"
     ]
    }
   ],
   "source": [
    "# calculating the accuracy\n",
    "correct = 0\n",
    "for x,y in zip(x_test, y_test):\n",
    "    output = predict(network, x)\n",
    "    if np.argmax(y) == np.argmax(output):\n",
    "        correct += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy: \",correct/len(x_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
